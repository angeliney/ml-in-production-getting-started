{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"../../data/lancedb-data/audio-lancedb\"\n",
    "db = lancedb.connect(uri)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "def test_method(test_fn, embed_fn=None):\n",
    "    queries_tbl = db.open_table(\"audio_example_queries\")\n",
    "    total_rows = queries_tbl.count_rows()\n",
    "    song_num_actual = []\n",
    "    song_num_retrieved = []\n",
    "\n",
    "    conditions = [\n",
    "        \"(offset == 0) and (pitch_shift == 0) and (time_stretch == 1.0)\",\n",
    "        \"(offset != 0) and (pitch_shift == 0) and (time_stretch == 1.0)\",\n",
    "        \"(offset == 0) and (pitch_shift != 0) and (time_stretch == 1.0)\",\n",
    "        \"(offset == 0) and (pitch_shift == 0) and (time_stretch != 1.0)\",\n",
    "        \"(offset == 0) and (pitch_shift != 0) and (time_stretch != 1.0)\",\n",
    "        \"(offset != 0) and (pitch_shift != 0) and (time_stretch != 1.0)\",\n",
    "    ]\n",
    "\n",
    "    for condition in conditions:\n",
    "        print(f\"Running test for condition: {condition}\")\n",
    "        filtered_tbl = queries_tbl.search().where(condition).select([\"song_num\", \"vector\"])\n",
    "\n",
    "        for _, row in filtered_tbl.to_pandas().iterrows():\n",
    "            if embed_fn:\n",
    "                row[\"vector\"] = embed_fn(row[\"vector\"])\n",
    "            song_num_actual.append(row[\"song_num\"])\n",
    "            retrieved_info_list = test_fn(row[\"vector\"]).to_pandas()\n",
    "\n",
    "            song_num_retrieved.append([retrieved_info[\"song_num\"] \n",
    "                                       for _, retrieved_info in retrieved_info_list.iterrows()])\n",
    "    return song_num_actual, song_num_retrieved\n",
    "\n",
    "\n",
    "def calculate_mrr(actual_songs, retrieved_songs):\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank (MRR) for a list of song retrievals.\n",
    "\n",
    "    Parameters:\n",
    "    actual_songs (list of int): A list of the actual song numbers.\n",
    "    retrieved_songs (list of list of int): A list of lists, where each inner list contains retrieved song numbers.\n",
    "\n",
    "    Returns:\n",
    "    float: The Mean Reciprocal Rank (MRR) score.\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for actual, retrieved in zip(actual_songs, retrieved_songs):\n",
    "        try:\n",
    "            # Find the rank (1-indexed) of the actual song in the retrieved list\n",
    "            rank = retrieved.index(actual) + 1\n",
    "            reciprocal_ranks.append(1 / rank)\n",
    "        except ValueError:\n",
    "            # If the actual song is not in the retrieved list, reciprocal rank is 0\n",
    "            reciprocal_ranks.append(0.0)\n",
    "\n",
    "    # Calculate the mean of the reciprocal ranks\n",
    "    return sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "\n",
    "\n",
    "def retrieval_recall(actual_songs, retrieved_songs):\n",
    "    in_retrieved = []\n",
    "    for actual, retrieved in zip(actual_songs, retrieved_songs):\n",
    "        in_retrieved.append(actual in retrieved)\n",
    "    \n",
    "    return np.sum(in_retrieved)/len(in_retrieved)\n",
    "\n",
    "\n",
    "def test_and_log(search_fn, embed_fn, search_metric, embedding):\n",
    "    actual, retrieved = test_method(search_fn, embed_fn)\n",
    "    mrr = calculate_mrr(actual, retrieved)\n",
    "    rr = retrieval_recall(actual, retrieved) \n",
    "    print(\"mrr\", mrr, \"rr\", rr)\n",
    "\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"children-song-dataset-retrieval\",\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "        \"embedding\": embedding,\n",
    "        \"retrieval\": search_metric,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    wandb.log({\"mrr\": mrr})\n",
    "    wandb.log({\"retrieval_recall\": rr})\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_tbl = db.open_table(\"audio_dataset\")\n",
    "\n",
    "def search(query_vector, db_tbl, metric=\"l2\"):\n",
    "    return db_tbl.search(query_vector).metric(metric).limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for search_metric in [\"l2\", \"cosine\", \"dot\"]:\n",
    "    test_and_log(lambda x: search(x, db_tbl, search_metric),\n",
    "                 None,\n",
    "                 search_metric,\n",
    "                 \"none\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = db_tbl.search().limit(1).select([\"song_num\", \"vector\"]).to_pandas().iloc[0][\"vector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(audio, sr=44100, aggregate=\"summary_stat\"):\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "\n",
    "    # Extract Chroma features\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "    \n",
    "    # Extract Mel-scaled spectrogram features\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n",
    "\n",
    "    if aggregate == \"summary_stat\":\n",
    "        # Aggregate the MFCCs across time\n",
    "        mfccs_mean = np.mean(mfccs, axis=1)\n",
    "        mfccs_std = np.std(mfccs, axis=1)\n",
    "        mfcc_embedding = np.concatenate([mfccs_mean, mfccs_std])\n",
    "\n",
    "        chroma_mean = np.mean(chroma, axis=1)\n",
    "        chroma_std = np.std(chroma, axis=1)\n",
    "        chroma_embedding = np.concatenate([chroma_mean, chroma_std])\n",
    "\n",
    "        mel_spectrogram_mean = np.mean(mel_spectrogram, axis=1)\n",
    "        mel_spectrogram_std = np.std(mel_spectrogram, axis=1)\n",
    "        mel_spectrogram_embedding = np.concatenate([mel_spectrogram_mean, mel_spectrogram_std])\n",
    "    \n",
    "    else:\n",
    "        # Flatten the MFCCs into a 1D array\n",
    "        mfcc_embedding = mfccs.flatten()\n",
    "        chroma_embedding = chroma.flatten()\n",
    "        mel_spectrogram_embedding = mel_spectrogram.flatten()\n",
    "   \n",
    "    return np.concatenate([mfcc_embedding, chroma_embedding, mel_spectrogram_embedding])\n",
    "\n",
    "# Example usage\n",
    "feat = extract_features(audio, aggregate=\"full\")\n",
    "print(feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df = db_tbl.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_lookup_data(embed_fn, df, db_name):\n",
    "    ## Re-embed the data with the new features\n",
    "    db_setup = False\n",
    "\n",
    "    batch_size = len(df)//5\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        print(i)\n",
    "        sound_arrays = []\n",
    "        for _, row in df.iloc[i:i+batch_size].iterrows():\n",
    "            sound_arrays.append(\n",
    "                {\n",
    "                    \"vector\": embed_fn(audio),\n",
    "                    \"sample_rate\": row[\"sample_rate\"],\n",
    "                    \"offset\": row[\"offset\"],\n",
    "                    \"pitch_shift\": row[\"pitch_shift\"],\n",
    "                    \"time_stretch\": row[\"time_stretch\"],\n",
    "                    \"song_num\": row[\"song_num\"],\n",
    "                    \"song_version\": row[\"song_version\"],\n",
    "                    \"chunk_num\": row[\"chunk_num\"],\n",
    "                    \"filename\": row[\"filename\"],\n",
    "                }\n",
    "            )\n",
    "    \n",
    "\n",
    "        if db_setup:\n",
    "            feat_tbl.add(sound_arrays)\n",
    "        else:\n",
    "            feat_tbl = db.create_table(db_name, data=sound_arrays)\n",
    "            db_setup = True\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_lookup_data(\n",
    "    lambda audio: extract_features(audio, aggregate=\"summary_stat\"),\n",
    "    audio_df,\n",
    "    \"audio_feat_eng_sumstat\"\n",
    ")\n",
    "\n",
    "embed_lookup_data(\n",
    "    lambda audio: extract_features(audio, aggregate=\"full\"),\n",
    "    audio_df,\n",
    "    \"audio_feat_eng_full\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumstat_tbl = db.open_table(\"audio_feat_eng_sumstat\")\n",
    "fullfeat_tbl = db.open_table(\"audio_feat_eng_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for search_metric in [\"l2\", \"cosine\", \"dot\"]:\n",
    "    test_and_log(\n",
    "        lambda x: search(x, sumstat_tbl, search_metric),\n",
    "        lambda audio: extract_features(audio, aggregate=\"summary_stat\"),\n",
    "        search_metric,\n",
    "        \"audio_feat_eng_sumstat\"\n",
    "    )\n",
    "\n",
    "    test_and_log(\n",
    "        lambda x: search(x, fullfeat_tbl, search_metric),\n",
    "        lambda audio: extract_features(audio, aggregate=\"full\"),\n",
    "        search_metric,\n",
    "        \"audio_feat_eng_full\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: try hubert or other audio pre-trained embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
